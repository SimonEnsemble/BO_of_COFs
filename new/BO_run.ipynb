{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "variable-triple",
   "metadata": {},
   "source": [
    "# BO runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "retained-equity",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from botorch.models import FixedNoiseGP, SingleTaskGP\n",
    "from gpytorch.kernels import ScaleKernel\n",
    "from gpytorch.mlls import ExactMarginalLogLikelihood\n",
    "from botorch import fit_gpytorch_model\n",
    "from botorch.acquisition.analytic import ExpectedImprovement\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import sys\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "romantic-mistake",
   "metadata": {},
   "outputs": [],
   "source": [
    "which_acquisition = \"EI\"\n",
    "# which_acquisition = \"max y_hat\"\n",
    "# which_acquisition = \"max sigma\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "hindu-startup",
   "metadata": {},
   "source": [
    "load data from `prepare_Xy.ipynb`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "flush-samba",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69839"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = pickle.load(open('inputs_and_outputs.pkl', 'rb'))['X']\n",
    "y = pickle.load(open('inputs_and_outputs.pkl', 'rb'))['y']\n",
    "y = np.reshape(y, (np.size(y), 1)) # for the GP\n",
    "nb_data = np.size(y)\n",
    "nb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mineral-update",
   "metadata": {},
   "source": [
    "convert to torch tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "south-coordination",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = torch.from_numpy(X)\n",
    "y = torch.from_numpy(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "weird-pleasure",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69839, 12])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "occupational-tracker",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([69839, 1])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y.size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "attended-secondary",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_unsqueezed = X.unsqueeze(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "criminal-support",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "69839"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unexpected-theorem",
   "metadata": {},
   "source": [
    "number of COFs for initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "varying-workstation",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_COFs_initialization = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "wanted-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "def bo_run(nb_iterations, verbose=False):\n",
    "    assert nb_iterations > nb_COFs_initialization\n",
    "    \n",
    "    # select initial COFs for training data randomly.\n",
    "    # idea is to keep populating this ids_acquired and return it for analysis.\n",
    "    ids_acquired = np.random.choice(np.arange((nb_data)), size=nb_COFs_initialization, replace=False)\n",
    "\n",
    "    # initialize acquired y, since it requires normalization\n",
    "    y_acquired = y[ids_acquired]\n",
    "    # standardize outputs\n",
    "    y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)\n",
    "    \n",
    "    for i in range(nb_COFs_initialization, nb_iterations):\n",
    "        print(\"iteration:\", i, end=\"\\r\")\n",
    "        # construct and fit GP model\n",
    "        model = SingleTaskGP(X[ids_acquired, :], y_acquired)\n",
    "        mll = ExactMarginalLogLikelihood(model.likelihood, model)\n",
    "        fit_gpytorch_model(mll)\n",
    "\n",
    "        # set up acquisition function\n",
    "        if which_acquisition == \"EI\":\n",
    "            acquisition_function = ExpectedImprovement(model, best_f=y_acquired.max().item())\n",
    "            \n",
    "            # compute aquisition function at each COF in the database. need to do in batches to avoid mem issues\n",
    "            batch_size = 35000\n",
    "            acquisition_values = torch.zeros((nb_data))\n",
    "            acquisition_values[:] = np.NaN # for safety\n",
    "            nb_batches = nb_data // batch_size\n",
    "            for ba in range(nb_batches+1):\n",
    "                id_start = ba * batch_size\n",
    "                id_end   = id_start + batch_size\n",
    "                if id_end > nb_data:\n",
    "                    id_end = nb_data\n",
    "                acquisition_values[id_start:id_end] = acquisition_function.forward(X_unsqueezed[id_start:id_end])\n",
    "            assert acquisition_values.isnan().sum().item() == 0 # so that all are filled properly.\n",
    "            del acquisition_function\n",
    "    #         acquisition_values = acquisition_function.forward(X_unsqueezed) # runs out of memory\n",
    "        elif which_acquisition == \"max y_hat\":\n",
    "            acquisition_values = model.posterior(X_unsqueezed).mean.squeeze()\n",
    "        elif which_acquisition == \"max sigma\":\n",
    "            acquisition_values = model.posterior(X_unsqueezed).variance.squeeze()\n",
    "        else:\n",
    "            raise Exception(\"not a valid acquisition function\")\n",
    "\n",
    "        # select COF to acquire with maximal aquisition value, which is not in the acquired set already\n",
    "        ids_sorted_by_aquisition = acquisition_values.argsort(descending=True)\n",
    "        for id_max_aquisition_all in ids_sorted_by_aquisition:\n",
    "            if not id_max_aquisition_all.item() in ids_acquired:\n",
    "                id_max_aquisition = id_max_aquisition_all.item()\n",
    "                break\n",
    "\n",
    "        # acquire this COF\n",
    "        ids_acquired = np.concatenate((ids_acquired, [id_max_aquisition]))\n",
    "        assert np.size(ids_acquired) == i + 1\n",
    "\n",
    "        # update y aquired; start over to normalize properly\n",
    "        del y_acquired\n",
    "        y_acquired = y[ids_acquired, :] # start over to normalize y properly\n",
    "        y_acquired = (y_acquired - torch.mean(y_acquired)) / torch.std(y_acquired)\n",
    "        \n",
    "        if verbose:\n",
    "            print(\"\\tacquired COF\", id_max_aquisition, \"with y = \", y[id_max_aquisition].item())\n",
    "            print(\"\\tbest y acquired:\", y[ids_acquired].max().item())\n",
    "        \n",
    "        del model\n",
    "        del mll\n",
    "        del acquisition_values\n",
    "        \n",
    "    assert np.size(ids_acquired) == nb_iterations\n",
    "    return ids_acquired"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "modern-elimination",
   "metadata": {},
   "source": [
    "`ids_acquired[r, i]` will give ID of COF acquired during iteration `i` from run `r`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "upset-bristol",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "RUN 0\n",
      "iteration: 22\r"
     ]
    }
   ],
   "source": [
    "bo_res = dict()\n",
    "bo_res['nb_runs']       = 50\n",
    "bo_res['nb_iterations'] = 250\n",
    "bo_res['ids_acquired'] = []\n",
    "for r in range(bo_res['nb_runs']):\n",
    "    print(\"\\n\\nRUN\", r)\n",
    "    t0 = time.time()\n",
    "    ids_acquired = bo_run(bo_res['nb_iterations'])\n",
    "    bo_res['ids_acquired'].append(ids_acquired)\n",
    "    print(\"took time t = \", (time.time() - t0) / 60, \"min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "tropical-poland",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bo_results' + which_acquisition + '.pkl', 'wb') as file:\n",
    "    pickle.dump(bo_res, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sexual-munich",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
